{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# since container uses UTC time, however, we are in UTC+8, \n",
    "# and for different timezones, you may need to change this value\n",
    "TZ_OFFSET = 8*3600 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stable_timepoint(x, y, scale_end_time=300, non_scaling_period=[200,280], stable_duration=100, stable_threshold=0.1):\n",
    "    \"\"\"\n",
    "    get the stable timepoint, where we define stable as the timepoint s:\n",
    "    1. s > scale_time\n",
    "    2. all the data in [s, s+stable_duration] is under (1+stable_threshold)*non-scaling-peak\n",
    "    3. non-scaling-peak is the max value in y[non_scaling_period[0]:non_scaling_period[1]]\n",
    "    \"\"\"\n",
    "    x,y = np.array(x), np.array(y)\n",
    "    non_scaling_peak = np.max( y[(x>non_scaling_period[0]) & (x<non_scaling_period[1])] )\n",
    "    stable_peak = (1+stable_threshold)*non_scaling_peak\n",
    "    post_scale_time = x[x>scale_end_time]\n",
    "\n",
    "    stable_time = -1\n",
    "    for t in post_scale_time:\n",
    "        if np.all(y[(x>=t) & (x<=t+stable_duration)] < stable_peak):\n",
    "            stable_time = t\n",
    "            break\n",
    "    if stable_time == -1:\n",
    "        print(\"No stable timepoint found, use the last timepoint\")\n",
    "        stable_time = x[-1]\n",
    "    return stable_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, folder, signal=None):\n",
    "        self.folder = folder\n",
    "        self.mechanism = folder.split(\"/\")[-1].split(\"-\")[0]\n",
    "        self.workload = folder.split(\"/\")[-1].split(\"-\")[1]\n",
    "\n",
    "        \n",
    "        ############# basic check #############\n",
    "        assert os.path.exists(folder)\n",
    "        assert os.path.exists(folder + f\"/{self.mechanism}-{self.workload}-statistics.log\")\n",
    "        assert os.path.exists(folder + f\"/jobmanager.log\")\n",
    "        # make sure \"All subtasks have acknowledged subscale completion\" exist in jobmanager.log\n",
    "        end = None\n",
    "        ana_job = 4\n",
    "        with open(folder + f\"/jobmanager.log\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if ana_job == 0:\n",
    "                    break\n",
    "                if \"All subtasks have acknowledged subscale completion\" in line:\n",
    "                    end = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})', line).group(1)\n",
    "                    # print(f\"Experiment {self.mechanism}-{self.workload} loaded: scale operation end time: {end}\")\n",
    "                    end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S,%f').timestamp() + TZ_OFFSET\n",
    "                    ana_job -= 1\n",
    "                elif \"Successfully loaded cache-capacit\" in line:\n",
    "                    # 2024-12-20 04:06:13,560 INFO  org.apache.flink.runtime.scale.ScaleConfig                   [] - Successfully loaded cache-capacity: 20000\n",
    "                    capacity = int(re.search(r'Successfully loaded cache-capacity: (\\d+)', line).group(1))\n",
    "                    ana_job -= 1\n",
    "                elif \"Successfully loaded subscale-reassignment:\" in line:\n",
    "                    emergency_ratio = float(re.search(r'emergency-ratio: ([\\d.]+),', line).group(1))\n",
    "                    fairness_weight = float(re.search(r'fairness-weight: ([\\d.]+)', line).group(1))\n",
    "                    # print(f\"  emergency_ratio: {emergency_ratio}, fairness_weight: {fairness_weight}\")\n",
    "                    ana_job -= 1\n",
    "                elif \"Successfully loaded state-sample-rate:\" in line:\n",
    "                    # Successfully loaded state-sample-rate: 2.5\n",
    "                    state_sample_rate = float(re.search(r'state-sample-rate: ([\\d.]+)', line).group(1))\n",
    "                    # keep first 1 digit\n",
    "                    state_sample_rate = round(state_sample_rate, 1)\n",
    "        # print(f\"Experiment {self.mechanism}-{self.workload} loaded: scale operation end time: {self.scale_operation_end_time}\")\n",
    "\n",
    "\n",
    "        ############# load basic information: mechanism-workload-statistics.log #############\n",
    "        latencies_list = None\n",
    "        self.no_scale = None\n",
    "        with open(folder + f\"/{self.mechanism}-{self.workload}-statistics.log\") as f:\n",
    "            for line in f:\n",
    "                if \"job_start_time\" in line:\n",
    "                    self.job_start_time = int(line.split(\": \")[1])\n",
    "                elif \"subscale_time\" in line:\n",
    "                    self.subscale_time = list(map(int, line.split(\": \")[1].strip(\"[]\\n\").split(\", \")))\n",
    "                elif \"scale_time\" in line:\n",
    "                    self.scale_trigger_time = int (line.split(\": \")[1])\n",
    "                elif \"host_info\" in line:\n",
    "                    self.scaling_host_info = eval(line.split(\": \", 1)[1])\n",
    "                elif \"inter_scheduler\" in line:\n",
    "                    inter_scheduler = line.split(\": \", 1)[1]\n",
    "                    inter_scheduler = inter_scheduler.strip()\n",
    "                elif \"intra_scheduler\" in line:\n",
    "                    intra_scheduler = line.split(\": \", 1)[1]\n",
    "                    intra_scheduler = intra_scheduler.strip()\n",
    "                elif \"latency\" in line:\n",
    "                    latencies_text = line.split(\": \", 1)[1]\n",
    "                    latencies_list = eval(latencies_text)\n",
    "                elif \"no_scale\" in line:\n",
    "                    self.no_scale = eval(line.split(\": \", 1)[1])\n",
    "        \n",
    "        assert (end is not None or self.no_scale is not None)\n",
    "        if end is not None:\n",
    "            self.scale_operation_end_time = end * 1000 # convert to ms\n",
    "\n",
    "        self.real_time_latencies = {}\n",
    "        if latencies_list is not None:\n",
    "            for timestamp, metrics in latencies_list:\n",
    "                if len(metrics) == 0:\n",
    "                    continue\n",
    "                relative_time = (timestamp*1000 - self.job_start_time) / 1000 # convert to relative time in seconds\n",
    "                for metric in metrics:\n",
    "                    relative_time = round(relative_time, 2)\n",
    "                    id = metric['id'].split(\"cl.\")[1]\n",
    "                    value = round(float(metric['value']), 4)\n",
    "                    # subtask_index = int(id.split(\".\")[0])\n",
    "                    metric_name = id.split(\".\")[1]\n",
    "                    # print(f\"  {relative_time}: {subtask_index}.{metric_name}: {value}\")\n",
    "                    if metric_name not in self.real_time_latencies:\n",
    "                        self.real_time_latencies[metric_name] = {}\n",
    "                    self.real_time_latencies[metric_name][relative_time] = value\n",
    "        \n",
    "        # sort by time\n",
    "        for key, value in self.real_time_latencies.items():\n",
    "            self.real_time_latencies[key] = dict(sorted(value.items(), key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "        \n",
    "        self.scaling_tms = set()\n",
    "        for host, info in self.scaling_host_info.items():\n",
    "            tm, _ =  info\n",
    "            self.scaling_tms.add(tm)\n",
    "        # print(f\"scaling_tms: {self.scaling_tms}\")\n",
    "        self.filesink_tms = set()\n",
    "        # for all \"docker-taskmanager-*.log\" files, find the corresponding taskmanager\n",
    "\n",
    "        for file in os.listdir(folder):\n",
    "            if file.startswith(\"docker-taskmanager-\") and file.endswith(\".log\"):\n",
    "                tm = file.split(\".\")[0]\n",
    "                if tm in self.scaling_tms:\n",
    "                    continue\n",
    "                with open(folder + \"/\" + file) as f:\n",
    "                    # if 'FileSink: Writer' in file, then it is a filesink tm\n",
    "                    if \"FileSink: Writer\" in f.read():\n",
    "                        self.filesink_tms.add(tm)\n",
    "        # print(f\"filesink_tms: {self.filesink_tms}\")\n",
    "        \n",
    "        \n",
    "        self.latency_loaded = False\n",
    "        self.suspend_loaded = False\n",
    "        self.latency_data = {}\n",
    "        self.state_transitions_loaded = False\n",
    "        self.throughput_loaded = False\n",
    "\n",
    "        ###################### intra-subscale ######################\n",
    "\n",
    "        if self.mechanism == \"drrs\":\n",
    "            self.test_env = {\n",
    "                \"capacity\": capacity,\n",
    "                \"fairness_weight\": fairness_weight,\n",
    "                \"emergency_ratio\": emergency_ratio,\n",
    "                \"inter_scheduler\": inter_scheduler,\n",
    "                \"intra_scheduler\": intra_scheduler,\n",
    "            }\n",
    "\n",
    "        if signal is None:\n",
    "            if self.no_scale:\n",
    "                self.signal = \"no_scale\"\n",
    "            # elif self.mechanism == \"drrs\":\n",
    "            #     self.signal =  inter_scheduler+\"-\"+intra_scheduler\n",
    "            else:\n",
    "                self.signal = self.mechanism + \"-\" + self.workload\n",
    "        else:\n",
    "            self.signal = signal\n",
    "    def get_subtask_by_tm(self, tm):\n",
    "        for host, info in self.scaling_host_info.items():\n",
    "            if info[0] == tm:\n",
    "                return host\n",
    "        raise Exception(f\"Taskmanager {tm} not found in scaling_host_info: {self.scaling_host_info}\")\n",
    "    \n",
    "    def load_latency(self, stable_during=150, stable_factor=0.2):\n",
    "        if self.no_scale is not None and (0, 0) in self.latency_data:\n",
    "            return self.latency_data\n",
    "        elif (stable_during, stable_factor) in self.latency_data:\n",
    "            return self.latency_data[(stable_during, stable_factor)]\n",
    "        latency = {}\n",
    "        latency_x = [] # output time\n",
    "        latency_y = [] # latency\n",
    "        for tm in self.filesink_tms:\n",
    "            with open(self.folder + f\"/{tm}.log\") as f:\n",
    "                for line in f:\n",
    "                    if 'FileSink: Writer' in line and 'creationTime' in line and 'outputTime' in line and 'Marker' in line:\n",
    "                        creation_time = int(re.search(r'creationTime=(\\d+),', line).group(1))\n",
    "                        output_time = int(re.search(r'outputTime=(\\d+)', line).group(1))\n",
    "                        if creation_time < 1e12:\n",
    "                            # print(f\"  something wrong with creation_time: {creation_time}, ignore\")\n",
    "                            continue\n",
    "\n",
    "                        assert creation_time < output_time\n",
    "                        # latency_x.append(output_time)\n",
    "                        # latency_y.append(output_time - creation_time)\n",
    "                        latency[output_time] = output_time - creation_time\n",
    "                        # if latency[output_time] > 7000 print current line\n",
    "        # sort by output time\n",
    "        sorted_latency = sorted(latency.items(), key=lambda x: x[0])\n",
    "        latency_x = [x[0] for x in sorted_latency]\n",
    "        latency_y = [x[1] for x in sorted_latency]\n",
    "        print(f\"Experiment {self.mechanism}-{self.workload}-{self.signal} loaded: {len(latency_y)} latency markers\")\n",
    "\n",
    "        latency_x, latency_y = np.array(latency_x), np.array(latency_y)\n",
    "        latency_x = (latency_x - self.job_start_time) / 1000 # convert to relative time in seconds\n",
    "        if self.no_scale is None:\n",
    "            r_scale_start_time = (self.scale_trigger_time - self.job_start_time) / 1000\n",
    "            r_scale_operation_end_time = (self.scale_operation_end_time - self.job_start_time) / 1000\n",
    "            print(f\"  scale : {r_scale_start_time} - {r_scale_operation_end_time} (only for scale operation completion)\")\n",
    "\n",
    "            stable_time = get_stable_timepoint(\n",
    "                latency_x, \n",
    "                latency_y, \n",
    "                scale_end_time=r_scale_operation_end_time, \n",
    "                non_scaling_period=[200,280],\n",
    "                stable_duration=stable_during, \n",
    "                stable_threshold=stable_factor)\n",
    "\n",
    "            print(f\"  stable time: {stable_time}\")\n",
    "        \n",
    "            # latency peak during scale start-stable time\n",
    "            peak_latency = np.max(latency_y[(r_scale_start_time < latency_x) & (latency_x < stable_time)])\n",
    "            average_latency = np.mean(latency_y[(r_scale_start_time < latency_x) & (latency_x < stable_time)])\n",
    "            print(f\"  re-stable time: {stable_time}, peak latency: {peak_latency}, average latency: {average_latency}\")\n",
    "    \n",
    "\n",
    "            latency = {\n",
    "                \"x\": latency_x,\n",
    "                \"y\": latency_y,\n",
    "                \"r_scale_start_time\": r_scale_start_time,\n",
    "                \"r_scale_operation_end_time\": r_scale_operation_end_time,\n",
    "                \"r_restable_time\": stable_time,\n",
    "                \"peak_latency\": peak_latency,\n",
    "                \"average_latency\": average_latency\n",
    "            }\n",
    "            self.latency_data[(stable_during, stable_factor)] = latency\n",
    "        else:\n",
    "            latency = {\n",
    "                \"x\": latency_x,\n",
    "                \"y\": latency_y,\n",
    "            }\n",
    "            print(f\"  no_scale: {len(latency_y)} latency markers\")\n",
    "            self.latency_data[(0, 0)] = latency\n",
    "        return latency\n",
    "\n",
    "    def load_suspend(self):\n",
    "        if self.suspend_loaded:\n",
    "            return\n",
    "        self.suspend_loaded = True\n",
    "        self.suspened = {}\n",
    "\n",
    "        for tm in self.scaling_tms:\n",
    "            suspended_ranges = []\n",
    "            with open(self.folder + f\"/{tm}.log\") as f:\n",
    "                suspended = False # normal suspend\n",
    "                start = -1\n",
    "                end = -1\n",
    "\n",
    "                mainThreadSuspend = False # main thread suspend(only for meces)\n",
    "                mainThreadStart = -1\n",
    "                mainThreadEnd = -1\n",
    "                for line in f.readlines():\n",
    "                    if \"mainTread is not available at\" in line:\n",
    "                        assert mainThreadSuspend == False\n",
    "                        mainThreadSuspend = True\n",
    "                        mainThreadStart = int(re.search(r'at (\\d+)', line).group(1))\n",
    "                    elif \"mainTread is available at\" in line:\n",
    "                        assert mainThreadSuspend == True\n",
    "                        mainThreadSuspend = False\n",
    "                        mainThreadEnd = int(re.search(r'at (\\d+)', line).group(1))\n",
    "                        suspended_ranges.append((mainThreadStart, mainThreadEnd))\n",
    "                    elif 'subtask subscale handler is available at' in line:\n",
    "                        # print(line)\n",
    "                        assert suspended == True\n",
    "                        suspended = False\n",
    "                        end = int(re.search(r'at (\\d+)', line).group(1))\n",
    "                        # print('start:', start, 'end:', end)\n",
    "                        assert start != -1 and end > start\n",
    "                        suspended_ranges.append((start, end))\n",
    "                        start, end = -1, -1\n",
    "                    elif 'subtask subscale handler is not available at' in line:\n",
    "                        assert suspended == False\n",
    "                        suspended = True\n",
    "                        assert start == -1 and end == -1\n",
    "                        start = int(re.search(r'at (\\d+)', line).group(1))\n",
    "            merged = []\n",
    "            suspended_ranges.sort(key=lambda x: x[0])\n",
    "            for start, end in suspended_ranges:\n",
    "                if not merged or start > merged[-1][1]:\n",
    "                    merged.append([start, end])\n",
    "                else:\n",
    "                    merged[-1][1] = max(merged[-1][1], end)\n",
    "            self.suspened[self.get_subtask_by_tm(tm)] = merged\n",
    "        print(f\"Experiment {self.mechanism}-{self.workload}-{self.signal} loaded: {len(self.suspened)} subtasks\")\n",
    "    \n",
    "    def load_state_info(self, print_info=False):\n",
    "        def print_info():\n",
    "            assert self.state_transitions_loaded\n",
    "            if \"approx_state_size\" in self.state_info:\n",
    "                # print(f\"  approx_state_size: {self.state_info['approx_state_size']}\")\n",
    "                # print the (approx_state_size, real_transition_size, Estimated_error ) for each state in each transition\n",
    "                avg_estimated_error = 0\n",
    "                errs = []\n",
    "                real_sizes = []\n",
    "                approx_sizes = []\n",
    "                for key, value in self.state_info[\"transitions\"].items():\n",
    "                    for key, _, size in value:\n",
    "                        approx_size = self.state_info[\"approx_state_size\"][key]/1024 # convert to KB\n",
    "                        estimated_error = np.abs(approx_size - size)/size * 100\n",
    "                        errs.append(estimated_error)\n",
    "                        print(f\"({key}: {int(approx_size)}KB, {size}KB, {estimated_error:.1f})\", end=\", \")\n",
    "                        real_sizes.append(size)\n",
    "                        approx_sizes.append(approx_size)\n",
    "                    print()\n",
    "                avg_estimated_error = np.mean(errs)\n",
    "                print(f\"  avg_estimated_error: {avg_estimated_error:.1f}\")\n",
    "\n",
    "                #  normalization\n",
    "                real_sizes = np.array(real_sizes)\n",
    "                approx_sizes = np.array(approx_sizes)\n",
    "                real_sizes = real_sizes / np.max(real_sizes)\n",
    "                approx_sizes = approx_sizes / np.max(approx_sizes)\n",
    "                # keep 2 digits:np.abs(real_sizes - approx_sizes)\n",
    "                print(f\"  diff: {np.round(real_sizes - approx_sizes, 2)}\")\n",
    "                # calculate the Scale Invariance Error\n",
    "                sie = np.mean(np.abs(real_sizes - approx_sizes))\n",
    "                print(f\"  Scale Invariance Error: {sie:.2f}\")\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        if self.state_transitions_loaded:\n",
    "            if print_info:\n",
    "                print_info()\n",
    "            return\n",
    "        self.state_transitions_loaded = True\n",
    "\n",
    "        # state transition order and state size\n",
    "        state = {}\n",
    "        for tm in self.scaling_tms:\n",
    "            with open(self.folder + f\"/{tm}.log\") as f:\n",
    "                subtask = self.get_subtask_by_tm(tm)\n",
    "                for line in f:\n",
    "                    if re.search(r'send state \\[\\d+\\] to task \\d+', line):\n",
    "                        # send state [0] to 1\n",
    "                        state_key = int(re.search(r'send state \\[(\\d+)\\] to task \\d+', line).group(1))\n",
    "                        target = int(re.search(r'send state \\[\\d+\\] to task (\\d+)', line).group(1))\n",
    "                        send_time = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3})', line).group(1)\n",
    "                        send_time = datetime.strptime(send_time, '%Y-%m-%d %H:%M:%S,%f').timestamp()\n",
    "                        send_time = send_time * 1000 - self.job_start_time\n",
    "                        send_time = send_time / 1000 # convert to relative time in seconds\n",
    "                        # print(f\"  send state {state_key} to {target} at {send_time}\")\n",
    "                        state[state_key] = {\"target\": target, \"send_time\": send_time, \"source\": subtask}\n",
    "                    if \"StateBuffer created on\" in line:\n",
    "                        # StateBuffer created on [15] with size 16999 KB\n",
    "                        state_key = int(re.search(r'\\[(\\d+)\\]', line).group(1))\n",
    "                        state_size = int(re.search(r'size (\\d+) KB', line).group(1))\n",
    "                        # print(f\"  StateBuffer created on {state_key} with size {state_size} KB\")\n",
    "                        state[state_key].update({\"size\": state_size})\n",
    "        # print(f\"Experiment {self.mechanism}-{self.workload}-{self.signal} loaded: {state} \")\n",
    "        # split by source-target\n",
    "        state_transitions = {}\n",
    "        for key, value in state.items():\n",
    "            source = value[\"source\"]\n",
    "            target = value[\"target\"]\n",
    "            if (source, target) not in state_transitions:\n",
    "                state_transitions[(source, target)] = []\n",
    "            state_transitions[(source, target)].append((key, value[\"send_time\"], value[\"size\"]))\n",
    "        # sort by send time\n",
    "        for key, value in state_transitions.items():\n",
    "            state_transitions[key] = sorted(value, key=lambda x: x[1])\n",
    "        # sort by source-target\n",
    "        state_transitions = dict(sorted(state_transitions.items(), key=lambda x: x[0]))\n",
    "        # self.state_transitions = state_transitions\n",
    "        self.state_info = {\"transitions\": state_transitions}\n",
    "        print(f\"Experiment {self.mechanism}-{self.workload}-{self.signal} loaded: {len(state_transitions)} state transitions\")\n",
    "\n",
    "        # read approximate state size from jobmanager.log(if exists)\n",
    "        with open(self.folder + f\"/jobmanager.log\") as f:\n",
    "            for line in f:\n",
    "                if \"Successfully collect state size\" in line:\n",
    "                    # 2024-12-20 04:28:19,580 INFO  org.apache.flink.runtime.scale.coordinator.ScaleCoordinator  [] - Successfully collect state size: {0=1267935, 1=1676696, 2=897459, 3=395127, 4=1193690, 5=11791440, 6=2981414, 7=4212991, 8=4252542, 9=1304190, 10=1374647, 11=4376856, 12=1803160, 13=1657700, 14=2007357, 15=514752, 16=1266435, 17=596949, 18=2059096, 19=8409912, 20=2163546, 21=2249976, 22=3744120, 23=9350923, 24=1270642, 25=753228, 26=525132, 27=1105585, 28=1539240, 29=2225800, 30=1025198, 31=1152918, 32=4526352, 33=752640, 34=606474, 35=4571868, 36=1588260, 37=636688, 38=783870, 39=995280, 40=918528, 41=885656, 42=2698798, 43=6490108, 44=10627160, 45=1034194, 46=753006, 47=5917274, 48=679764, 49=1516437, 50=3681960, 51=4868460, 52=3603418, 53=2379480, 54=5322614, 55=617004, 56=958518, 57=552600, 58=963144, 59=2015007, 60=474300, 61=760000, 62=1004585, 63=1585668, 64=3645808, 65=688128, 66=14544200, 67=421038, 68=854220, 69=498295, 70=1925720, 71=15214320, 72=1477527, 73=1820312, 74=1188039, 75=1187646, 76=533750, 77=1355088, 78=9948050, 79=2548764, 80=1345582, 81=4431180, 82=947085, 83=2036600, 84=1300684, 85=2202928, 86=960042, 87=1634640, 88=3771690, 89=641690, 90=6510996, 91=846765, 92=1112584, 93=412080, 94=1713800, 95=1436790, 96=422280, 97=746118, 98=976854, 99=1138480, 100=4838680, 101=1844500, 102=899640, 103=1433660, 104=2024064, 105=798510, 106=6821685, 107=8454181, 108=2327688, 109=4024658, 110=2785104, 111=13193030, 112=528176, 113=504900, 114=8876014, 115=674082, 116=4147233, 117=2789670, 118=668520, 119=982902, 120=7131000, 121=1944111, 122=374852, 123=3394800, 124=6654228, 125=1543248, 126=8142644, 127=2142976}\n",
    "                    state_size_str = re.search(r'Successfully collect state size: (.+)', line).group(1)\n",
    "                    state_size_str = state_size_str.replace('=', ':')\n",
    "                    approx_state_size = eval(state_size_str)\n",
    "                    # sort by state key\n",
    "                    approx_state_size = dict(sorted(approx_state_size.items(), key=lambda x: x[0]))\n",
    "                    self.state_info[\"approx_state_size\"] = approx_state_size\n",
    "\n",
    "\n",
    "        if print_info:\n",
    "            print_info()\n",
    "    \n",
    "    def load_throughput(self):\n",
    "        if self.throughput_loaded:\n",
    "            return\n",
    "        assert os.path.exists(self.folder + f\"/throughputs.log\")\n",
    "        throughput = {}\n",
    "        with open(self.folder + f\"/throughputs.log\") as f:\n",
    "            for line in f:\n",
    "                timestamp = int(re.search(r'^(\\d+):', line).group(1))\n",
    "                timestamp = (timestamp - self.job_start_time) / 1000 # convert to relative time in seconds\n",
    "                metric_str = re.search(r'\\[(.*)\\]', line).group(0)\n",
    "                if metric_str == '[]':\n",
    "                    continue\n",
    "                metric_str = metric_str.replace(\"'\", '\"')\n",
    "                metric_json = json.loads(metric_str.strip())\n",
    "                for id, value in [(item['id'], item['value']) for item in metric_json]:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except Exception as e:\n",
    "                        print('Cannot convert to int:', value)\n",
    "                        continue\n",
    "                    # timestamp: 1730286916984 id: 1.Source__Bid_Source.numRecordsInPerSecond value: 4.6\n",
    "                    subtask= int(re.search(r'^(\\d+)\\.', id).group(1))\n",
    "                    operator = re.search(r'\\Source__(\\w+)_', id).group(1)\n",
    "                    if operator not in throughput:\n",
    "                        throughput[operator] = {}\n",
    "                    if subtask not in throughput[operator]:\n",
    "                        throughput[operator][subtask] = {}\n",
    "                    throughput[operator][subtask][timestamp] = value\n",
    "        # merge all subtasks into operator-level\n",
    "        # notice: simply sum all subtasks' throughput is not correct, because the timestamp may not be aligned\n",
    "        # so we need to align the timestamp\n",
    "        operator_throughput = {}\n",
    "        if len(throughput) == 1:\n",
    "            throughput = throughput[list(throughput.keys())[0]]\n",
    "            all_timestamps = set()\n",
    "            for subtask, data in throughput.items():\n",
    "                all_timestamps.update(data.keys())\n",
    "            all_timestamps = sorted(list(all_timestamps))\n",
    "            for subtask, data in throughput.items():\n",
    "                timestamps = np.array(list(data.keys()))\n",
    "                values = np.array(list(data.values()))\n",
    "                interp = interp1d(timestamps, values, kind='cubic', fill_value='extrapolate')\n",
    "                interpolated_values = interp(all_timestamps)\n",
    "                interpolated_values = np.clip(interpolated_values, 0, None)\n",
    "                for i, timestamp in enumerate(all_timestamps):\n",
    "                    if timestamp not in operator_throughput:\n",
    "                        operator_throughput[timestamp] = 0\n",
    "                    operator_throughput[timestamp] += interpolated_values[i]\n",
    "            \n",
    "            if self.workload == 'q7':\n",
    "                # multiply by 2, because the the input take 1 as 2\n",
    "                for timestamp in operator_throughput:\n",
    "                    operator_throughput[timestamp] *= 2\n",
    "\n",
    "        else:\n",
    "            # add all operators' throughput\n",
    "            # can not simply sum, because the timestamp may not be aligned\n",
    "            # so we need to align the timestamp\n",
    "            all_timestamps = set()\n",
    "            for operator, subtasks in throughput.items():\n",
    "                for subtask, data in subtasks.items():\n",
    "                    all_timestamps.update(data.keys())\n",
    "            all_timestamps = sorted(list(all_timestamps))\n",
    "            for operator, subtasks in throughput.items():\n",
    "                for subtask, data in subtasks.items():\n",
    "                    timestamps = np.array(list(data.keys()))\n",
    "                    values = np.array(list(data.values()))\n",
    "                    interp = interp1d(timestamps, values, kind='cubic', fill_value='extrapolate')\n",
    "                    interpolated_values = interp(all_timestamps)\n",
    "                    interpolated_values = np.clip(interpolated_values, 0, None)\n",
    "                    for i, timestamp in enumerate(all_timestamps):\n",
    "                        if timestamp not in operator_throughput:\n",
    "                            operator_throughput[timestamp] = 0\n",
    "                        operator_throughput[timestamp] += interpolated_values[i]\n",
    "        print(operator_throughput)\n",
    "        self.throughput_loaded = True\n",
    "        self.throughput = operator_throughput\n",
    "    \n",
    "    def load_throughput_for_fwc(self):\n",
    "    \n",
    "        output_times = []\n",
    "        with open(self.folder + f\"/output.log\") as f:\n",
    "            # print(f\"loading {self.folder}/output.log with half_scale_time: {(half_scale_time- self.job_start_time)/1000}\")\n",
    "            for line in f:\n",
    "                # expectedGenTime\n",
    "                gen_time = int(re.search(r'expectedGenTime: (\\d+)', line).group(1))\n",
    "                processing_time = int(re.search(r'processedTime: (\\d+)', line).group(1))\n",
    "                output_times.append( (processing_time-self.job_start_time)//1000 ) # ms to s with offset\n",
    "        \n",
    "        output_times.sort()\n",
    "        # print(f\"output_times count: {len(output_times)}, first: {output_times[0]}, last: {output_times[-1]}\")\n",
    "        end_time = output_times[-1]//60\n",
    "        throughput = np.zeros(11)\n",
    "        for t in output_times:\n",
    "            throughput[t//60] += 1\n",
    "        # remove the first and last 1 minute\n",
    "        # print(throughput)\n",
    "        average_throughput = throughput[1:-1].mean()\n",
    "        self.average_throughput = average_throughput\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
